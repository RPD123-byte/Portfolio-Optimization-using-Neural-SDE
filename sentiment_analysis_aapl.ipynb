{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This column was originally published on RealMoney on Jan. 18 at 8:42 a.m. ET. It's being republished as a bonus for TheStreet.com readers. For more information about subscribing to RealMoney, please click here. Let's talk about sentiment. More specifically, let's discuss the Investors Intelligence readings that came out Wednesday. The percentage of bulls sank to 50.5%, well off the high of 59.8% in early December. Yes, this confirms that the bullish ranks are thinning. But please, do not confuse this with too much bearishness. There's not too much bearishness -- just less bullishness. The percentage of bears in the survey has remained steadily in the 20% range for months now, so anyone who wants to fuss over a rise of 2 percentage points is grasping at straws, in my opinion. However, the massive rise in the percentage of folks looking for a correction was noteworthy. We last discussed this number in early December. I went through great pains to show that when this reading gets below 17% as it did then, the market rarely keeps on soaring. The best outcome is often a sideways market. For six weeks afterward, the Nasdaq went nowhere. The S&P 500 was around 1420 then, and today it stands a \"whopping\" 10 points higher at 1430. The fact that it took six weeks to move 10 points is not what I'd call a ringing endorsement of a rally. I'd point out that few were looking for a correction in December, yet that's exactly what we got: a market going nowhere or a correction.  Page 2 of 2 A Cure for Complacency But now the number of correction-minded folks has risen to 27.4%, according to the survey. It's funny how a market that went sideways for six weeks got people to jump into the correction camp. Why don't they look for a correction when the market is soaring? Wasn't the time to look for a correction in mid-December, not early January? I suppose that is why sentiment matters: Human nature rarely changes. I'm sure I could conjure up some statistical way of saying that when the percentage of correction-minded folks gets this high, the market does something specific. But I really can't. Instead, what you should keep in mind as you look at these readings is that correction-minded folks are not bears. They are bulls who are looking for a correction. So if we add up the bulls and the correction-minded people, you can see from the chart that bullishness is alive and well in the market. It peaked in late December at 80.4% and is now at 77.9%. That's like one person moving from bull to bear. If you want to make a fuss over that, then by all means, please do. Me? I will say bullishness is thinning, but it is not gone. Let's move on to the American Association of Individual Investors readings, just out this morning. The bulls are at 57.58%, a reading not seen since last January, which was for all intents and purposes the Nasdaq high for the first half of the year. (If you don't believe me, look at when stocks like Apple (AAPL:Nasdaq) and SanDisk (SNDK:Nasdaq) peaked.) This tells me that the little guy is back in as a believer. So he sat out the entire rally and now likes it? Another thing that concerns me is that the put/call ratio was higher Tuesday than it was Wednesday. Tuesday's market was barely down, so why was the ratio higher than it was Wednesday, when the market, especially the Nasdaq, was clearly weak? That says to me that people were more worried about Intel's (INTC:Nasdaq) earnings than they were about Apple's earnings. Bulls should like worrywarts, and Wednesday they went into hiding. The 10-day moving average of the put/call ratio, which I showed here on Tuesday morning, is back at the bottom of the range, usually the point where we tend to go back down again. Thus, where Tuesday folks were scared, Wednesday they showed too much complacency. A few down days -- just what I've been looking for! -- might cure that complacency. In the past few months, that's about all it has taken. But I'll be monitoring it closely. Overbought/Oversold Oscillators For more explanation of these indicators, check out The Chartist's primer. Load-Date: January 20, 2007\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=\"user\",\n",
    "        port=\"port\",\n",
    "        host=\"localhost\",\n",
    "        database=\"name\"\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # The table sentiment_analysis_data_aapl contains tens of thousands of news articles about AAPL\n",
    "    select_query1 = \"SELECT * FROM sentiment_analysis_data_aapl;\"\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(select_query1)\n",
    "    # Fetch all records\n",
    "    records_aapl = cursor.fetchall()\n",
    "    print(records_aapl[700][3])\n",
    "    \n",
    "except Exception as error:\n",
    "    print(f\"Failed to fetch records: {error}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dropped successfully.\n",
      "Table Created Successfully\n"
     ]
    }
   ],
   "source": [
    "# Connect to database\n",
    "connection = psycopg2.connect(\n",
    "    user=\"user\",\n",
    "    port=\"port\",\n",
    "    host=\"localhost\",\n",
    "    database=\"name\"\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "drop_table_query = \"DROP TABLE IF EXISTS sentiment_data_aapl;\"\n",
    "cursor.execute(drop_table_query)\n",
    "connection.commit()\n",
    "print(\"Table dropped successfully.\")\n",
    "\n",
    "create_table_query = '''CREATE TABLE sentiment_data_aapl (\n",
    "    ID SERIAL PRIMARY KEY,\n",
    "    Title TEXT,\n",
    "    Time_Published TEXT,\n",
    "    Sentiment_Score REAL\n",
    "); '''\n",
    "\n",
    "cursor.execute(create_table_query)\n",
    "connection.commit()\n",
    "print(\"Table Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment of Article 1 is 1.1111111111111112 at January 12, 2007 Friday 10:40 AM EST\n",
      "Average Sentiment of Article 2 is 1.1111111111111112 at January 16, 2007 Tuesday 8:46 PM EST\n",
      "Average Sentiment of Article 3 is 1.1111111111111112 at January 17, 2007 Wednesday 12:55 PM EST\n",
      "Average Sentiment of Article 4 is 1.1111111111111112 at January 15, 2007 Monday 6:26 PM EST\n",
      "Average Sentiment of Article 5 is 1.1111111111111112 at January 16, 2007 Tuesday 7:25 PM EST\n",
      "Average Sentiment of Article 6 is 1.2972972972972974 at January 1, 2007 Monday 11:55 AM EST\n",
      "Average Sentiment of Article 7 is 1.0 at January 30, 2007 Tuesday 08:54 AM EST\n",
      "Average Sentiment of Article 8 is 0.4583333333333333 at January 19, 2007 Friday 9:16 AM EST\n",
      "Average Sentiment of Article 9 is 1.0158730158730158 at January 17, 2007 Wednesday 9:30 AM EST\n",
      "Average Sentiment of Article 10 is 1.0588235294117647 at January 19, 2007 Friday 09:26 AM EST\n",
      "Average Sentiment of Article 11 is 0.9183673469387755 at January 22, 2007 Monday 9:48 AM EST\n",
      "Average Sentiment of Article 12 is 0.8225806451612904 at January 19, 2007 Friday 11:42 AM EST\n",
      "Average Sentiment of Article 13 is 1.1076923076923078 at January 11, 2007 Thursday 9:32 AM EST\n",
      "Average Sentiment of Article 14 is 0.6444444444444445 at January 10, 2007 Wednesday 9:16 AM EST\n",
      "Average Sentiment of Article 15 is 1.1111111111111112 at January 15, 2007 Monday 3:22 PM EST\n",
      "Average Sentiment of Article 16 is 1.1111111111111112 at January 16, 2007 Tuesday 6:25 PM EST\n",
      "Average Sentiment of Article 17 is 1.1111111111111112 at January 17, 2007 Wednesday 3:42 PM EST\n",
      "Average Sentiment of Article 18 is 1.1111111111111112 at January 17, 2007 Wednesday 11:55 AM EST\n",
      "Average Sentiment of Article 19 is 0.6428571428571429 at January 9, 2007 Tuesday 4:25 PM EST\n",
      "Average Sentiment of Article 20 is 0.975 at January 11, 2007 Thursday 07:40 AM EST\n",
      "Average Sentiment of Article 21 is 1.2926829268292683 at January 4, 2007 Thursday 11:55 AM EST\n",
      "Average Sentiment of Article 22 is 1.0204081632653061 at January 3, 2007 Wednesday 10:36 AM EST\n",
      "Average Sentiment of Article 23 is 0.9019607843137255 at January 9, 2007 Tuesday 9:43 AM EST\n",
      "Average Sentiment of Article 24 is 1.1818181818181819 at January 18, 2007 Thursday 12:39 PM EST\n",
      "Average Sentiment of Article 25 is 0.7777777777777778 at January 19, 2007 Friday 1:33 PM EST\n",
      "Average Sentiment of Article 26 is 0.7652173913043478 at January 22, 2007 Monday 2:34 PM EST\n",
      "Average Sentiment of Article 27 is 0.9 at January 12, 2007 Friday 6:32 AM EST\n",
      "Average Sentiment of Article 28 is 0.6923076923076923 at January 10, 2007 Wednesday 11:55 AM EST\n",
      "Average Sentiment of Article 29 is 0.7 at January 18, 2007 Thursday 4:41 PM EST\n",
      "Average Sentiment of Article 30 is 0.5833333333333334 at January 18, 2007 Thursday 8:59 AM EST\n",
      "Average Sentiment of Article 31 is 0.875 at January 25, 2007 Thursday 6:46 AM EST\n",
      "Average Sentiment of Article 32 is 0.8333333333333334 at January 16, 2007 Tuesday 5:54 PM EST\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i, (article, date) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(article_text, article_date)):\n\u001b[1;32m     42\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(article)  \u001b[39m# Split the article into sentences\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     sentiment_scores \u001b[39m=\u001b[39m [get_sentiment(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]  \u001b[39m# Get sentiment for each sentence\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     avg_sentiment \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(sentiment_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(sentiment_scores)  \u001b[39m# Average the sentiment scores\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     cursor\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO sentiment_data_aapl (ID, Sentiment_Score, Time_Published) VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                    (i, avg_sentiment, date))\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i, (article, date) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(article_text, article_date)):\n\u001b[1;32m     42\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(article)  \u001b[39m# Split the article into sentences\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     sentiment_scores \u001b[39m=\u001b[39m [get_sentiment(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]  \u001b[39m# Get sentiment for each sentence\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     avg_sentiment \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(sentiment_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(sentiment_scores)  \u001b[39m# Average the sentiment scores\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     cursor\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO sentiment_data_aapl (ID, Sentiment_Score, Time_Published) VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                    (i, avg_sentiment, date))\n",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     29\u001b[0m inputs \u001b[39m=\u001b[39m preprocess(text)\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     32\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     33\u001b[0m probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1314\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1314\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   1315\u001b[0m     input_ids,\n\u001b[1;32m   1316\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1318\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1319\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1320\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1321\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1322\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1323\u001b[0m )\n\u001b[1;32m   1325\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1326\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1084\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1076\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1077\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1082\u001b[0m )\n\u001b[0;32m-> 1084\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1085\u001b[0m     embedding_output,\n\u001b[1;32m   1086\u001b[0m     attention_mask,\n\u001b[1;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1088\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         rel_embeddings,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    521\u001b[0m         next_kv,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    524\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    525\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    526\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    530\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:373\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m     attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n\u001b[1;32m    372\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 373\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m (layer_output, att_matrix)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:339\u001b[0m, in \u001b[0;36mDebertaV2Output.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 339\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    340\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    341\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    user=\"user\",\n",
    "    port=\"port\",\n",
    "    host=\"localhost\",\n",
    "    database=\"name\"\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"nickmuchi/deberta-v3-base-finetuned-finance-text-classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)  # Moved to device\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Inference function for a single sentence\n",
    "def get_sentiment(text):\n",
    "    inputs = preprocess(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    label_id = torch.argmax(probabilities).item()\n",
    "    return label_id\n",
    "\n",
    "# Your articles\n",
    "article_text = [record[3] for record in records_aapl]\n",
    "article_date = [record[2] for record in records_aapl]\n",
    "article_title = [record[1] for record in records_aapl]\n",
    "\n",
    "# Iterate over articles\n",
    "for i, (article, date, title) in enumerate(zip(article_text, article_date, article_title)):\n",
    "    sentences = sent_tokenize(article)  # Split the article into sentences\n",
    "    sentiment_scores = [get_sentiment(sentence) for sentence in sentences]  # Get sentiment for each sentence\n",
    "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)  # Average the sentiment scores\n",
    "    cursor.execute(\"INSERT INTO sentiment_data_aapl (ID, Title, Sentiment_Score, Time_Published) VALUES (%s, %s, %s, %s)\",\n",
    "                   (i, title, avg_sentiment, date))\n",
    "    connection.commit()\n",
    "    print(f\"Average Sentiment of Article {i+1} is {avg_sentiment} at {date} for {title}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
