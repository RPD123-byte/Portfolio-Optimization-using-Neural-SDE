{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim Cramer, in a special edition of TheStreet.com TV's Wall Street Confidential Web video, explained what not to do when shorting a stock. Cramer said that whether you are a professional or amateur, do not use only common stock to short. \"It doesn't work,\" he said. For people who are addicted to shorting common stock, Cramer suggested buying the Whirlpool(WHR:NYSE) 85 calls as insurance and shorting common against it. Do not use regular common stock to short \"in this kind of blowoff to the upside,\" Cramer stressed. Or, you can just buy puts instead. \"If you're going to short, you can only do it with puts, you cannot do it with common,\" Cramer said. (Shorting is borrowing stock and selling it for the proceeds, with the expectation it will go down and eventually can be bought back at a lower price. The repurchased shares then must be returned to the lender. A call is an option to buy a stock at a predetermined price by a certain date; a put is the opposite, to sell at a specific price.)  Page 2 of 2 TheStreet.com TV Recap: A Walk on the Short Side Cramer looked at what some shorts were thinking in three stocks: Amazon (AMZN:Nasdaq), Whirlpool (WHR:NYSE) and F5 Networks (FFIV:Nasdaq). Those investors, he said, had theses that there were problems with the stocks that would not be solved this quarter, because there were no catalysts. A lot of people had called and emailed him, Cramer said, to tell him that he should go negative on F5 Networks because it had been acting like a \"dog.\" People believe there's no way that a \"traffic coordinator\" on the Web -- F5 Networks -- could continue to grow, he said. But that's not a good reason to short a stock. As for Amazon, Cramer said he had \"mistakenly\" pointed out that because it was up 70% since August, maybe it was time to get off. But this, again, is not a good reason to sell, he said. Market-players are shorting Whirlpool because its last quarter was not that good. \"In all three of these cases, the guidance from these companies from the previous quarter was not good,\" Cramer continued. Therefore, \"short-sellers felt that this quarter had to be bad.\" However, people should not have trusted these companies' guidance, he said. \"They didn't know how to guide going into the quarter, why would we trust their guidance going out?\" The companies were \"sandbagging,\" Cramer said. If investors really believed that Amazon was going to go down, they didn't need to see it this quarter, he explained. People should not have used common stock to short Amazon or Whirlpool. \"I would've bought the Whirlpool 90 puts for a couple of bucks, or maybe the 85 puts if you thought it was going to fall apart,\" Cramer said. \"With Amazon, the 40 puts would've been right.\" And F5 Networks \"is a stock that is up so monstrously that two strikes down would've been right,\" he said. Load-Date: April 28, 2007\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=\"user\",\n",
    "        port=\"port\",\n",
    "        host=\"localhost\",\n",
    "        database=\"name\"\n",
    "    )\n",
    "    \n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # The table sentiment_analysis_data_amzn contains tens of thousands of news articles about AMZN\n",
    "    select_query1 = \"SELECT * FROM sentiment_analysis_data_amzn;\"\n",
    "\n",
    "    cursor.execute(select_query1)\n",
    "    \n",
    "    # Fetch all records\n",
    "    records_amzn = cursor.fetchall()\n",
    "    print(records_amzn[700][3])\n",
    "    \n",
    "except Exception as error:\n",
    "    print(f\"Failed to fetch records: {error}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table dropped successfully.\n",
      "Table Created Successfully\n"
     ]
    }
   ],
   "source": [
    "connection = psycopg2.connect(\n",
    "    user=\"user\",\n",
    "    port=\"port\",\n",
    "    host=\"localhost\",\n",
    "    database=\"name\"\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "drop_table_query = \"DROP TABLE IF EXISTS sentiment_data_amzn;\"\n",
    "cursor.execute(drop_table_query)\n",
    "connection.commit()\n",
    "print(\"Table dropped successfully.\")\n",
    "\n",
    "create_table_query = '''CREATE TABLE sentiment_data_amzn (\n",
    "    ID SERIAL PRIMARY KEY,\n",
    "    Title TEXT,\n",
    "    Time_Published TEXT,\n",
    "    Sentiment_Score REAL\n",
    "); '''\n",
    "\n",
    "cursor.execute(create_table_query)\n",
    "connection.commit()\n",
    "print(\"Table Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentiment of Article 1 is 0.9166666666666666 at January 31, 2007 Wednesday 11:01 PM EST\n",
      "Average Sentiment of Article 2 is 1.3636363636363635 at January 22, 2007 Monday 10:39 AM EST\n",
      "Average Sentiment of Article 3 is 0.16666666666666666 at January 3, 2007 Wednesday 10:23 AM EST\n",
      "Average Sentiment of Article 4 is 1.5454545454545454 at January 3, 2007 Wednesday 12:24 PM EST\n",
      "Average Sentiment of Article 5 is 1.1 at January 29, 2007 Monday 1:30 PM GMT\n",
      "Average Sentiment of Article 6 is 1.0444444444444445 at January 18, 2007 Thursday 14:46 PM EST\n",
      "Average Sentiment of Article 7 is 1.0263157894736843 at January 16, 2007 Tuesday 15:55 PM EST\n",
      "Average Sentiment of Article 8 is 1.1578947368421053 at January 24, 2007 Wednesday 4:46 PM EST\n",
      "Average Sentiment of Article 9 is 1.0416666666666667 at January 16, 2007 Tuesday\n",
      "Average Sentiment of Article 10 is 1.14 at January 29, 2007 Monday 07:45 AM EST\n",
      "Average Sentiment of Article 11 is 0.9 at January 27, 2007 Saturday 9:55 AM EST\n",
      "Average Sentiment of Article 12 is 0.9 at January 29, 2007 Monday 5:58 AM EST\n",
      "Average Sentiment of Article 13 is 0.9 at January 28, 2007 Sunday 10:25 AM EST\n",
      "Average Sentiment of Article 14 is 0.9 at January 26, 2007 Friday 2:38 PM EST\n",
      "Average Sentiment of Article 15 is 0.9230769230769231 at January 22, 2007 Monday 8:32 AM EST\n",
      "Average Sentiment of Article 16 is 1.0476190476190477 at January 25, 2007 Thursday 4:15 PM GMT\n",
      "Average Sentiment of Article 17 is 1.0384615384615385 at January 11, 2007 Thursday 2:31 PM GMT\n",
      "Average Sentiment of Article 18 is 0.9444444444444444 at January 19, 2007 Friday 11:55 AM EST\n",
      "Average Sentiment of Article 19 is 1.0 at January 22, 2007 Monday 7:16 AM EST\n",
      "Average Sentiment of Article 20 is 1.0 at January 29, 2007 Monday 2:00 PM GMT\n",
      "Average Sentiment of Article 21 is 0.7272727272727273 at January 11, 2007 Thursday 09:23 am EDT\n",
      "Average Sentiment of Article 22 is 1.1764705882352942 at January 31, 2007 Wednesday 4:24 PM EST\n",
      "Average Sentiment of Article 23 is 1.0 at January 22, 2007 Monday 11:00 AM GMT\n",
      "Average Sentiment of Article 24 is 1.3846153846153846 at January 3, 2007 Wednesday 9:01 AM EST\n",
      "Average Sentiment of Article 25 is 0.8 at January 26, 2007 Friday 5:24 PM EST\n",
      "Average Sentiment of Article 26 is 0.6551724137931034 at January 18, 2007 Thursday 09:27 AM EST\n",
      "Average Sentiment of Article 27 is 1.0909090909090908 at January 3, 2007 Wednesday 2:11 PM GMT\n",
      "Average Sentiment of Article 28 is 0.9807692307692307 at January 30, 2007 Tuesday\n",
      "Average Sentiment of Article 29 is 1.08 at January 3, 2007 Wednesday 2:28 PM GMT\n",
      "Average Sentiment of Article 30 is 1.0909090909090908 at January 3, 2007 Wednesday 2:11 PM GMT\n",
      "Average Sentiment of Article 31 is 1.08 at January 3, 2007 Wednesday 2:12 PM GMT\n",
      "Average Sentiment of Article 32 is 1.1724137931034482 at January 10, 2007 Wednesday 18:05 PM EST\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i, (article, date) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(article_text, article_date)):\n\u001b[1;32m     42\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(article)  \u001b[39m# Split the article into sentences\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     sentiment_scores \u001b[39m=\u001b[39m [get_sentiment(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]  \u001b[39m# Get sentiment for each sentence\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     avg_sentiment \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(sentiment_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(sentiment_scores)  \u001b[39m# Average the sentiment scores\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     cursor\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO sentiment_data_amzn (ID, Sentiment_Score, Time_Published) VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                    (i, avg_sentiment, date))\n",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m i, (article, date) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(article_text, article_date)):\n\u001b[1;32m     42\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(article)  \u001b[39m# Split the article into sentences\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     sentiment_scores \u001b[39m=\u001b[39m [get_sentiment(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]  \u001b[39m# Get sentiment for each sentence\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     avg_sentiment \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(sentiment_scores) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(sentiment_scores)  \u001b[39m# Average the sentiment scores\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     cursor\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO sentiment_data_amzn (ID, Sentiment_Score, Time_Published) VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                    (i, avg_sentiment, date))\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     29\u001b[0m inputs \u001b[39m=\u001b[39m preprocess(text)\n\u001b[1;32m     30\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 31\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m     32\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     33\u001b[0m probabilities \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1314\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1314\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   1315\u001b[0m     input_ids,\n\u001b[1;32m   1316\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1318\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1319\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1320\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1321\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1322\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1323\u001b[0m )\n\u001b[1;32m   1325\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1326\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1084\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m   1076\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1077\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m   1082\u001b[0m )\n\u001b[0;32m-> 1084\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1085\u001b[0m     embedding_output,\n\u001b[1;32m   1086\u001b[0m     attention_mask,\n\u001b[1;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1088\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1089\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1090\u001b[0m )\n\u001b[1;32m   1091\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     output_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         rel_embeddings,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     output_states \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    521\u001b[0m         next_kv,\n\u001b[1;32m    522\u001b[0m         attention_mask,\n\u001b[1;32m    523\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    524\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    525\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    526\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    530\u001b[0m     output_states, att_m \u001b[39m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:373\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m     attention_output, att_matrix \u001b[39m=\u001b[39m attention_output\n\u001b[1;32m    372\u001b[0m intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 373\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    375\u001b[0m     \u001b[39mreturn\u001b[39;00m (layer_output, att_matrix)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:339\u001b[0m, in \u001b[0;36mDebertaV2Output.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states, input_tensor):\n\u001b[0;32m--> 339\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    340\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    341\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    user=\"user\",\n",
    "    port=\"port\",\n",
    "    host=\"localhost\",\n",
    "    database=\"name\"\n",
    ")\n",
    "\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"nickmuchi/deberta-v3-base-finetuned-finance-text-classification\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)  # Moved to device\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Inference function for a single sentence\n",
    "def get_sentiment(text):\n",
    "    inputs = preprocess(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    label_id = torch.argmax(probabilities).item()\n",
    "    return label_id\n",
    "\n",
    "# Your articles\n",
    "article_text = [record[3] for record in records_amzn]\n",
    "article_date = [record[2] for record in records_amzn]\n",
    "article_title = [record[1] for record in records_amzn]\n",
    "\n",
    "# Iterate over articles\n",
    "for i, (article, date, title) in enumerate(zip(article_text, article_date, article_title)):\n",
    "    sentences = sent_tokenize(article)  # Split the article into sentences\n",
    "    sentiment_scores = [get_sentiment(sentence) for sentence in sentences]  # Get sentiment for each sentence\n",
    "    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)  # Average the sentiment scores\n",
    "    cursor.execute(\"INSERT INTO sentiment_data_amzn (ID, Title, Sentiment_Score, Time_Published) VALUES (%s, %s, %s, %s)\",\n",
    "                   (i, title, avg_sentiment, date))\n",
    "    connection.commit()\n",
    "    print(f\"Average Sentiment of Article {i+1} is {avg_sentiment} at {date} for {title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
